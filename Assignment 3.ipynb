{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading data\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# For visualizing\n",
    "import plotly.express as px\n",
    "\n",
    "# For model building\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTIDX(Dataset):\n",
    "    def __init__(self, image_url, label_url, transform=None):\n",
    "        # Load labels\n",
    "        response = requests.get(label_url)\n",
    "        with gzip.open(io.BytesIO(response.content), 'rb') as lbpath:\n",
    "            labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "        # Load images\n",
    "        response = requests.get(image_url)\n",
    "        with gzip.open(io.BytesIO(response.content), 'rb') as imgpath:\n",
    "            images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 28, 28)\n",
    "\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(torch.tensor(image, dtype=torch.float32))\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = FashionMNISTIDX('https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-images-idx3-ubyte.gz',\n",
    "                          'https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-labels-idx1-ubyte.gz')\n",
    "\n",
    "test = FashionMNISTIDX('https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/t10k-images-idx3-ubyte.gz',\n",
    "                       'https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFashionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFashionNet, self).__init__()\n",
    "        # Flatten the input image\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layer that maps the 28x28 input to 10 output classes\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image\n",
    "        x = self.flatten(x)\n",
    "        # Apply the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        # Apply log softmax to the output\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleFashionNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some training parameters\n",
    "learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "# Define our loss function\n",
    "#   This one works for multiclass problems\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)  # Get the total number of samples in the dataset\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch, (X, y) in enumerate(dataloader):  # Iterate over batches\n",
    "        X, y = X.float(), y  # Convert inputs to float\n",
    "        pred = model(X)  # Get model predictions\n",
    "        loss = loss_fn(pred, y)  # Calculate the loss\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        if batch % 100 == 0:  # Print loss every 100 batches\n",
    "            loss, current = loss.item(), batch * len(X)  # Get the current loss and number of samples processed\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")  # Print the loss and progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)  # Total number of samples in the dataset\n",
    "    num_batches = len(dataloader)  # Total number of batches\n",
    "    test_loss, correct = 0, 0  # Initialize test loss and correct predictions count\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for X, y in dataloader:  # Iterate over batches\n",
    "            X, y = X.float(), y  # Convert inputs to float\n",
    "            pred = model(X)  # Get model predictions\n",
    "            test_loss += loss_fn(pred, y).item()  # Accumulate loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # Count correct predictions\n",
    "\n",
    "    test_loss /= num_batches  # Calculate average loss\n",
    "    correct /= size  # Calculate accuracy\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")  # Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 118.771233 [    0/60000]\n",
      "loss: 1409.493896 [ 6400/60000]\n",
      "loss: 4240.417969 [12800/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghaith\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:285: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3014.708496 [19200/60000]\n",
      "loss: 5032.420898 [25600/60000]\n",
      "loss: 694.342834 [32000/60000]\n",
      "loss: 281.754639 [38400/60000]\n",
      "loss: 1292.497070 [44800/60000]\n",
      "loss: 800.562500 [51200/60000]\n",
      "loss: 534.525146 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 988.187885 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 571.791992 [    0/60000]\n",
      "loss: 2424.322998 [ 6400/60000]\n",
      "loss: 1273.923828 [12800/60000]\n",
      "loss: 459.013855 [19200/60000]\n",
      "loss: 1364.981812 [25600/60000]\n",
      "loss: 753.615479 [32000/60000]\n",
      "loss: 1255.692505 [38400/60000]\n",
      "loss: 868.303284 [44800/60000]\n",
      "loss: 695.403076 [51200/60000]\n",
      "loss: 617.057617 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 973.054800 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 858.107239 [    0/60000]\n",
      "loss: 767.272827 [ 6400/60000]\n",
      "loss: 2632.609131 [12800/60000]\n",
      "loss: 1228.200317 [19200/60000]\n",
      "loss: 604.835999 [25600/60000]\n",
      "loss: 902.173218 [32000/60000]\n",
      "loss: 630.164001 [38400/60000]\n",
      "loss: 590.805115 [44800/60000]\n",
      "loss: 5214.740723 [51200/60000]\n",
      "loss: 600.839905 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 3356.676141 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 4402.457031 [    0/60000]\n",
      "loss: 783.770813 [ 6400/60000]\n",
      "loss: 874.259521 [12800/60000]\n",
      "loss: 1328.422363 [19200/60000]\n",
      "loss: 1611.927490 [25600/60000]\n",
      "loss: 1347.742676 [32000/60000]\n",
      "loss: 807.243652 [38400/60000]\n",
      "loss: 797.064087 [44800/60000]\n",
      "loss: 471.888306 [51200/60000]\n",
      "loss: 1173.635376 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 2544.635245 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2274.635254 [    0/60000]\n",
      "loss: 2336.833008 [ 6400/60000]\n",
      "loss: 609.259705 [12800/60000]\n",
      "loss: 708.926270 [19200/60000]\n",
      "loss: 1233.839355 [25600/60000]\n",
      "loss: 1123.454102 [32000/60000]\n",
      "loss: 1382.799561 [38400/60000]\n",
      "loss: 663.918457 [44800/60000]\n",
      "loss: 1416.585083 [51200/60000]\n",
      "loss: 1376.267456 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 2508.390005 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2679.938965 [    0/60000]\n",
      "loss: 1269.743652 [ 6400/60000]\n",
      "loss: 664.595825 [12800/60000]\n",
      "loss: 896.733093 [19200/60000]\n",
      "loss: 2081.563965 [25600/60000]\n",
      "loss: 634.972778 [32000/60000]\n",
      "loss: 996.328003 [38400/60000]\n",
      "loss: 153.689224 [44800/60000]\n",
      "loss: 2375.448486 [51200/60000]\n",
      "loss: 2600.479004 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 1674.412357 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1406.930664 [    0/60000]\n",
      "loss: 631.738770 [ 6400/60000]\n",
      "loss: 3907.700195 [12800/60000]\n",
      "loss: 674.019958 [19200/60000]\n",
      "loss: 1084.360107 [25600/60000]\n",
      "loss: 914.285767 [32000/60000]\n",
      "loss: 3495.585938 [38400/60000]\n",
      "loss: 1685.211792 [44800/60000]\n",
      "loss: 1295.526367 [51200/60000]\n",
      "loss: 689.587830 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 1037.061714 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1093.470215 [    0/60000]\n",
      "loss: 1597.245728 [ 6400/60000]\n",
      "loss: 565.375610 [12800/60000]\n",
      "loss: 331.312805 [19200/60000]\n",
      "loss: 1094.920044 [25600/60000]\n",
      "loss: 1625.912964 [32000/60000]\n",
      "loss: 718.166992 [38400/60000]\n",
      "loss: 1255.763794 [44800/60000]\n",
      "loss: 1798.107422 [51200/60000]\n",
      "loss: 2080.329102 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 2285.971374 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2016.384033 [    0/60000]\n",
      "loss: 2239.545898 [ 6400/60000]\n",
      "loss: 240.441132 [12800/60000]\n",
      "loss: 995.386841 [19200/60000]\n",
      "loss: 323.190643 [25600/60000]\n",
      "loss: 397.398987 [32000/60000]\n",
      "loss: 530.453613 [38400/60000]\n",
      "loss: 2110.808594 [44800/60000]\n",
      "loss: 354.468445 [51200/60000]\n",
      "loss: 1056.137939 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 1803.114006 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2164.310059 [    0/60000]\n",
      "loss: 2955.389648 [ 6400/60000]\n",
      "loss: 655.921143 [12800/60000]\n",
      "loss: 1826.884521 [19200/60000]\n",
      "loss: 1708.988037 [25600/60000]\n",
      "loss: 701.026184 [32000/60000]\n",
      "loss: 989.151978 [38400/60000]\n",
      "loss: 425.332825 [44800/60000]\n",
      "loss: 791.529175 [51200/60000]\n",
      "loss: 1078.267822 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 2150.097960 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1499.672119 [    0/60000]\n",
      "loss: 1610.670532 [ 6400/60000]\n",
      "loss: 738.620056 [12800/60000]\n",
      "loss: 1649.418457 [19200/60000]\n",
      "loss: 888.698669 [25600/60000]\n",
      "loss: 3385.081299 [32000/60000]\n",
      "loss: 1065.111938 [38400/60000]\n",
      "loss: 957.615540 [44800/60000]\n",
      "loss: 2876.919434 [51200/60000]\n",
      "loss: 816.872620 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 2163.222216 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2066.248779 [    0/60000]\n",
      "loss: 2822.659668 [ 6400/60000]\n",
      "loss: 1308.044800 [12800/60000]\n",
      "loss: 1685.578369 [19200/60000]\n",
      "loss: 789.677979 [25600/60000]\n",
      "loss: 2239.296387 [32000/60000]\n",
      "loss: 337.429382 [38400/60000]\n",
      "loss: 3183.453369 [44800/60000]\n",
      "loss: 2193.929199 [51200/60000]\n",
      "loss: 2112.715088 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 1196.116069 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 861.724609 [    0/60000]\n",
      "loss: 538.345520 [ 6400/60000]\n",
      "loss: 2955.059570 [12800/60000]\n",
      "loss: 434.996429 [19200/60000]\n",
      "loss: 966.225281 [25600/60000]\n",
      "loss: 1724.880859 [32000/60000]\n",
      "loss: 425.328278 [38400/60000]\n",
      "loss: 582.017700 [44800/60000]\n",
      "loss: 972.422729 [51200/60000]\n",
      "loss: 658.755493 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 2029.924894 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 750.708557 [    0/60000]\n",
      "loss: 1032.230103 [ 6400/60000]\n",
      "loss: 658.899414 [12800/60000]\n",
      "loss: 957.268127 [19200/60000]\n",
      "loss: 1448.040649 [25600/60000]\n",
      "loss: 1589.046753 [32000/60000]\n",
      "loss: 469.276886 [38400/60000]\n",
      "loss: 1636.970703 [44800/60000]\n",
      "loss: 922.776855 [51200/60000]\n",
      "loss: 600.887939 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1422.432689 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1126.901733 [    0/60000]\n",
      "loss: 2523.407715 [ 6400/60000]\n",
      "loss: 906.160339 [12800/60000]\n",
      "loss: 609.714844 [19200/60000]\n",
      "loss: 1054.527832 [25600/60000]\n",
      "loss: 979.555054 [32000/60000]\n",
      "loss: 1183.535034 [38400/60000]\n",
      "loss: 2002.406494 [44800/60000]\n",
      "loss: 621.976562 [51200/60000]\n",
      "loss: 1071.264771 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1635.636019 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1201.240723 [    0/60000]\n",
      "loss: 990.977905 [ 6400/60000]\n",
      "loss: 218.088181 [12800/60000]\n",
      "loss: 1486.398560 [19200/60000]\n",
      "loss: 1280.053955 [25600/60000]\n",
      "loss: 355.045471 [32000/60000]\n",
      "loss: 457.601624 [38400/60000]\n",
      "loss: 1092.119507 [44800/60000]\n",
      "loss: 1776.764893 [51200/60000]\n",
      "loss: 897.653503 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1721.431345 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2239.922607 [    0/60000]\n",
      "loss: 1522.503662 [ 6400/60000]\n",
      "loss: 1086.595215 [12800/60000]\n",
      "loss: 858.475769 [19200/60000]\n",
      "loss: 269.403015 [25600/60000]\n",
      "loss: 921.798645 [32000/60000]\n",
      "loss: 978.290283 [38400/60000]\n",
      "loss: 1103.871094 [44800/60000]\n",
      "loss: 478.490814 [51200/60000]\n",
      "loss: 780.786499 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 4379.163299 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 3017.478760 [    0/60000]\n",
      "loss: 529.052246 [ 6400/60000]\n",
      "loss: 549.682800 [12800/60000]\n",
      "loss: 1753.390869 [19200/60000]\n",
      "loss: 612.514832 [25600/60000]\n",
      "loss: 2788.003174 [32000/60000]\n",
      "loss: 1072.369019 [38400/60000]\n",
      "loss: 1117.874634 [44800/60000]\n",
      "loss: 1023.553894 [51200/60000]\n",
      "loss: 949.532471 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 847.184129 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 822.994690 [    0/60000]\n",
      "loss: 1365.292725 [ 6400/60000]\n",
      "loss: 307.954651 [12800/60000]\n",
      "loss: 768.612122 [19200/60000]\n",
      "loss: 1297.014404 [25600/60000]\n",
      "loss: 1582.017212 [32000/60000]\n",
      "loss: 620.108215 [38400/60000]\n",
      "loss: 337.399170 [44800/60000]\n",
      "loss: 375.723145 [51200/60000]\n",
      "loss: 1121.378784 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 935.760191 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1191.867920 [    0/60000]\n",
      "loss: 781.436829 [ 6400/60000]\n",
      "loss: 1410.021362 [12800/60000]\n",
      "loss: 751.694458 [19200/60000]\n",
      "loss: 1156.725220 [25600/60000]\n",
      "loss: 370.291473 [32000/60000]\n",
      "loss: 435.587677 [38400/60000]\n",
      "loss: 784.661499 [44800/60000]\n",
      "loss: 1104.982788 [51200/60000]\n",
      "loss: 466.569397 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1266.758804 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 458.909698 [    0/60000]\n",
      "loss: 649.438721 [ 6400/60000]\n",
      "loss: 972.672852 [12800/60000]\n",
      "loss: 1551.883667 [19200/60000]\n",
      "loss: 147.658112 [25600/60000]\n",
      "loss: 3256.575195 [32000/60000]\n",
      "loss: 1155.479004 [38400/60000]\n",
      "loss: 1050.553711 [44800/60000]\n",
      "loss: 835.089905 [51200/60000]\n",
      "loss: 827.986206 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 1047.992879 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 237.491821 [    0/60000]\n",
      "loss: 480.314484 [ 6400/60000]\n",
      "loss: 589.018677 [12800/60000]\n",
      "loss: 586.664185 [19200/60000]\n",
      "loss: 963.600708 [25600/60000]\n",
      "loss: 816.781616 [32000/60000]\n",
      "loss: 1328.368652 [38400/60000]\n",
      "loss: 732.816162 [44800/60000]\n",
      "loss: 480.449280 [51200/60000]\n",
      "loss: 829.247131 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1951.199686 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1293.189453 [    0/60000]\n",
      "loss: 822.561157 [ 6400/60000]\n",
      "loss: 1261.479736 [12800/60000]\n",
      "loss: 1551.216919 [19200/60000]\n",
      "loss: 655.015930 [25600/60000]\n",
      "loss: 974.053101 [32000/60000]\n",
      "loss: 489.009918 [38400/60000]\n",
      "loss: 391.728424 [44800/60000]\n",
      "loss: 401.858093 [51200/60000]\n",
      "loss: 937.039185 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 6319.296489 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 8937.638672 [    0/60000]\n",
      "loss: 638.935974 [ 6400/60000]\n",
      "loss: 480.257629 [12800/60000]\n",
      "loss: 1423.984131 [19200/60000]\n",
      "loss: 433.741516 [25600/60000]\n",
      "loss: 2362.396729 [32000/60000]\n",
      "loss: 789.997437 [38400/60000]\n",
      "loss: 495.212891 [44800/60000]\n",
      "loss: 1303.100830 [51200/60000]\n",
      "loss: 1089.525513 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 1344.660030 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1279.644287 [    0/60000]\n",
      "loss: 786.975464 [ 6400/60000]\n",
      "loss: 353.311829 [12800/60000]\n",
      "loss: 775.960815 [19200/60000]\n",
      "loss: 463.546051 [25600/60000]\n",
      "loss: 1417.091797 [32000/60000]\n",
      "loss: 2090.625000 [38400/60000]\n",
      "loss: 913.964355 [44800/60000]\n",
      "loss: 765.655884 [51200/60000]\n",
      "loss: 1068.585083 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 954.419804 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 756.489380 [    0/60000]\n",
      "loss: 2152.447021 [ 6400/60000]\n",
      "loss: 1118.667603 [12800/60000]\n",
      "loss: 1268.573364 [19200/60000]\n",
      "loss: 204.571915 [25600/60000]\n",
      "loss: 1260.850342 [32000/60000]\n",
      "loss: 880.192383 [38400/60000]\n",
      "loss: 416.021790 [44800/60000]\n",
      "loss: 590.002197 [51200/60000]\n",
      "loss: 1106.965576 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 1269.664341 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1126.774414 [    0/60000]\n",
      "loss: 326.219666 [ 6400/60000]\n",
      "loss: 1040.535400 [12800/60000]\n",
      "loss: 2170.352539 [19200/60000]\n",
      "loss: 553.240112 [25600/60000]\n",
      "loss: 1817.051514 [32000/60000]\n",
      "loss: 535.626221 [38400/60000]\n",
      "loss: 1015.591553 [44800/60000]\n",
      "loss: 1341.675049 [51200/60000]\n",
      "loss: 584.175903 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 805.325438 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 772.152405 [    0/60000]\n",
      "loss: 971.650146 [ 6400/60000]\n",
      "loss: 626.924744 [12800/60000]\n",
      "loss: 986.536987 [19200/60000]\n",
      "loss: 978.236267 [25600/60000]\n",
      "loss: 659.416748 [32000/60000]\n",
      "loss: 1210.296631 [38400/60000]\n",
      "loss: 521.399292 [44800/60000]\n",
      "loss: 509.102661 [51200/60000]\n",
      "loss: 583.969116 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 1273.645369 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 498.578735 [    0/60000]\n",
      "loss: 627.339233 [ 6400/60000]\n",
      "loss: 1326.654541 [12800/60000]\n",
      "loss: 980.507935 [19200/60000]\n",
      "loss: 722.612671 [25600/60000]\n",
      "loss: 964.610657 [32000/60000]\n",
      "loss: 1519.835449 [38400/60000]\n",
      "loss: 700.360352 [44800/60000]\n",
      "loss: 299.090332 [51200/60000]\n",
      "loss: 620.498657 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1286.179287 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 988.128601 [    0/60000]\n",
      "loss: 205.855469 [ 6400/60000]\n",
      "loss: 533.265808 [12800/60000]\n",
      "loss: 671.565918 [19200/60000]\n",
      "loss: 691.900513 [25600/60000]\n",
      "loss: 1695.661377 [32000/60000]\n",
      "loss: 601.771240 [38400/60000]\n",
      "loss: 717.705811 [44800/60000]\n",
      "loss: 1003.265381 [51200/60000]\n",
      "loss: 1009.518982 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 1517.302595 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is predicted to be a 4, and is labeled as 2\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "image, truth = test_dataloader.dataset.__getitem__(1)\n",
    "image = torch.tensor(image).float().unsqueeze(0)\n",
    "\n",
    "pred = model(image).argmax()\n",
    "\n",
    "print(f\"This image is predicted to be a {pred.item()}, and is labeled as {truth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our model\n",
    "EPOCH = epochs\n",
    "PATH = \"model.pt\"\n",
    "\n",
    "# The save function creates a binary storing all our data for us\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghaith\\AppData\\Local\\Temp\\ipykernel_25708\\2088115640.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(PATH)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"model.pt\"\n",
    "\n",
    "# Create a new \"blank\" model to load our information into\n",
    "model = SimpleFashionNet()\n",
    "\n",
    "# Recreate our optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Load back all of our data from the file\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "EPOCH = checkpoint['epoch']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
